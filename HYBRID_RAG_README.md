# ğŸ§  Hybrid RAG System

A **Factual and Reliable Question Answering System** using Hybrid Retrieval-Augmented Generation with Quantized LLMs.

## âœ¨ Features

- **ğŸ”„ Hybrid Retrieval**: Combines BM25 (sparse) + BGE-M3 (dense) embeddings
- **ğŸ¯ Smart Reranking**: Uses BGE Reranker for improved result quality  
- **âœ… Answer Verification**: Checks generated answers against source documents
- **ğŸš¨ Hallucination Detection**: Identifies and flags potentially inaccurate information
- **ğŸ“Š Interactive UI**: Streamlit frontend with evaluation capabilities
- **âš¡ Fast API**: RESTful backend for integration

## ğŸ› ï¸ Models Used

- **Generator**: `qwen2.5:7b-instruct-q3_k_m` (Quantized 7B instruction-tuned model)
- **Retriever**: `bge-m3` (Multilingual embedding model)  
- **Reranker**: `xitao/bge-reranker-v2-m3` (Cross-encoder reranking)

## ğŸ“Š Dataset

- **Source**: SQuAD v1.1 (87,599 question-answer pairs)
- **Processed**: 18,894 document chunks with 512 token chunks
- **Indexing**: BM25 + ChromaDB vector store

## ğŸš€ Quick Start

### Prerequisites

1. **Ollama** must be running:
   ```bash
   ollama serve
   ```

2. **Required models** (should already be installed):
   ```bash
   ollama pull qwen2.5:7b-instruct-q3_k_m
   ollama pull bge-m3  
   ollama pull xitao/bge-reranker-v2-m3
   ```

### Launch System

```bash
# Single command to start everything
python launch_rag_system.py
```

This will start:
- **Backend API**: http://localhost:8000
- **Frontend UI**: http://localhost:8501  
- **API Documentation**: http://localhost:8000/docs

### Manual Launch (Alternative)

```bash
# Terminal 1: Start backend
cd backend
uvicorn app:app --host 0.0.0.0 --port 8000

# Terminal 2: Start frontend  
cd frontend
streamlit run app.py --server.port 8501
```

## ğŸ–¥ï¸ User Interface

### ğŸ’¬ Chat Interface
- Ask questions and get verified answers
- Real-time answer verification
- View retrieved source documents
- Hallucination risk assessment

### ğŸ” Document Search  
- Search knowledge base directly
- Compare BM25 vs Dense retrieval scores
- Fusion score visualization

### ğŸ“Š Answer Evaluation
- Evaluate answer quality and consistency
- Claim-by-claim verification analysis  
- Confidence scoring

## ğŸ”§ API Endpoints

- `GET /health` - System health check
- `POST /search` - Document retrieval only
- `POST /generate` - Answer generation from context
- `POST /rag` - Complete RAG pipeline  
- `POST /evaluate` - Answer quality evaluation
- `GET /stats` - System statistics

## âš™ï¸ Configuration

Edit `config/settings.py` to adjust:

```python
RETRIEVAL_CONFIG = {
    "fusion_alpha": 0.3,      # BM25 vs Dense weight  
    "top_k_final": 5,         # Documents to return
    "rerank_top_k": 3         # Documents after reranking
}
```

## ğŸ“ˆ System Architecture

```
Query â†’ Hybrid Retrieval â†’ Generation â†’ Verification â†’ Response
        â†“                   â†“            â†“
    BM25 + Dense        Qwen2.5      Consistency 
    + Reranking         7B Model     Checker
```

## ğŸ¯ Research Goals

This system implements the methodology from our research proposal:

1. **Hybrid Retrieval** - Combine sparse + dense methods
2. **Verification-Driven Generation** - Validate against sources  
3. **Hallucination Mitigation** - Detect and flag inaccuracies
4. **Human Evaluation Interface** - Quality assessment tools

## ğŸ“ Evaluation Criteria

- **Relevance**: How well does the answer address the query?
- **Faithfulness**: Is the response grounded in evidence?
- **Hallucination**: Risk of fabricated information
- **Usefulness**: Overall helpfulness and completeness

## ğŸ” Example Usage

```python
# Using the API directly
import requests

response = requests.post("http://localhost:8000/rag", json={
    "query": "What is the University of Notre Dame?",
    "top_k": 5,
    "include_verification": True
})

result = response.json()
print(f"Answer: {result['answer']}")
print(f"Verification: {result['verification']['overall_verdict']}")
```

## ğŸ“Š Performance Stats

- **Documents**: 18,894 chunks from SQuAD
- **Vocabulary**: 96,926 unique terms  
- **Average Response Time**: ~2-3 seconds
- **Verification Accuracy**: Measured by human evaluation

## ğŸ› ï¸ Development

### Project Structure
```
â”œâ”€â”€ backend/           # FastAPI application
â”œâ”€â”€ frontend/          # Streamlit interface  
â”œâ”€â”€ config/            # Configuration files
â”œâ”€â”€ data/              # Datasets and indices
â”œâ”€â”€ launch_rag_system.py  # System launcher
â””â”€â”€ README.md
```

### Key Components
- `hybrid_retriever.py` - Combines BM25 + dense retrieval
- `answer_verifier.py` - Checks answer consistency  
- `data_processor.py` - Processes SQuAD dataset
- `app.py` (backend) - FastAPI REST API
- `app.py` (frontend) - Streamlit UI

## ğŸ¤ Contributing

This is a research implementation. Areas for improvement:

- [ ] Add more evaluation metrics
- [ ] Implement conversation memory
- [ ] Support for document upload
- [ ] Multi-language support  
- [ ] Advanced reranking strategies

## ğŸ“„ License

This project is for research and educational purposes.

---

**Built with â¤ï¸ for reliable and factual question answering**
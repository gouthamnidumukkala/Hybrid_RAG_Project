{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79756cb",
   "metadata": {},
   "source": [
    "#### Checking Access with Olama API Key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48b12f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama API Key found. Access is verified.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Ollama API key from environment\n",
    "OLLAMA_API_KEY = os.getenv('OLLAMA_API_KEY')\n",
    "\n",
    "if OLLAMA_API_KEY:\n",
    "    print(\"Ollama API Key found. Access is verified.\")\n",
    "else:\n",
    "    print(\"Ollama API Key not found. Please set it in the .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a38e75",
   "metadata": {},
   "source": [
    "#### Creating Chatbot with Llama3.2:1b-text-q2_K Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee49376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\"models\":[{\"name\":\"nomic-embed-text:latest\",\"model\":\"nomic-embed-text:latest\",\"modified_at\":\"2025-11-29T19:36:44.252285305+05:30\",\"size\":274302450,\"digest\":\"0a109f422b47e3a30ba2b10eca18548e944e8a23073ee3f3e947efcf3c45e59f\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"nomic-bert\",\"families\":[\"nomic-bert\"],\"parameter_size\":\"137M\",\"quantization_level\":\"F16\"}},{\"name\":\"llama3.2:1b-text-q2_K\",\"model\":\"llama3.2:1b-text-q2_K\",\"modified_at\":\"2025-11-29T19:03:59.571508051+05:30\",\"size\":580884282,\"digest\":\"1e2a8fc966e5af6b0fdfadf643cd06ace111a3fb715b28d803abfa4bdbc48b58\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"1.2B\",\"quantization_level\":\"Q2_K\"}},{\"name\":\"qwen2.5vl:7b\",\"model\":\"qwen2.5vl:7b\",\"modified_at\":\"2025-06-14T22:17:12.359138026+05:30\",\"size\":5969245856,\"digest\":\"5ced39dfa4bac325dc183dd1e4febaa1c46b3ea28bce48896c8e69c1e79611cc\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"qwen25vl\",\"families\":[\"qwen25vl\"],\"pa\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# if you need to override default\n",
    "os.environ.setdefault(\"OLLAMA_HOST\", \"127.0.0.1:11434\")   # set if you used a different port\n",
    "os.environ.setdefault(\"OLLAMA_API_KEY\", \"your_api_key_here\")  # if required\n",
    "\n",
    "resp = requests.get(\"http://127.0.0.1:11434/api/tags\", timeout=5)\n",
    "print(resp.status_code)\n",
    "print(resp.text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4275beef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"models\": [\n",
      "    {\n",
      "      \"name\": \"llama3.2:1b-text-q2_K\",\n",
      "      \"model\": \"llama3.2:1b-text-q2_K\",\n",
      "      \"modified_at\": \"2025-11-29T19:03:59.571508051+05:30\",\n",
      "      \"size\": 580884282,\n",
      "      \"digest\": \"1e2a8fc966e5af6b0fdfadf643cd06ace111a3fb715b28d803abfa4bdbc48b58\",\n",
      "      \"details\": {\n",
      "        \"parent_model\": \"\",\n",
      "        \"format\": \"gguf\",\n",
      "        \"family\": \"llama\",\n",
      "        \"families\": [\n",
      "          \"llama\"\n",
      "        ],\n",
      "        \"parameter_size\": \"1.2B\",\n",
      "        \"quantization_level\": \"Q2_K\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"qwen2.5vl:7b\",\n",
      "      \"model\": \"qwen2.5vl:7b\",\n",
      "      \"modified_at\": \"2025-06-14T22:17:12.359138026+05:30\",\n",
      "      \"size\": 5969245856,\n",
      "      \"digest\": \"5ced39dfa4bac325dc183dd1e4febaa1c46b3ea28bce48896c8e69c1e79611cc\",\n",
      "      \"details\": {\n",
      "        \"parent_model\": \"\",\n",
      "        \"format\": \"gguf\",\n",
      "        \"family\": \"qwen25vl\",\n",
      "        \"families\": [\n",
      "          \"qwen25vl\"\n",
      "        ],\n",
      "        \"parameter_size\": \"8.3B\",\n",
      "        \"quantization_level\": \"Q4_K_M\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = resp.text\n",
    "response_json = json.loads(response)\n",
    "print(json.dumps(response_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43d1e927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(model='llama3.2:1b-text-q2_K', modified_at=datetime.datetime(2025, 11, 29, 19, 3, 59, 571508, tzinfo=TzInfo(19800)), digest='1e2a8fc966e5af6b0fdfadf643cd06ace111a3fb715b28d803abfa4bdbc48b58', size=580884282, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='1.2B', quantization_level='Q2_K')),\n",
       " Model(model='qwen2.5vl:7b', modified_at=datetime.datetime(2025, 6, 14, 22, 17, 12, 359138, tzinfo=TzInfo(19800)), digest='5ced39dfa4bac325dc183dd1e4febaa1c46b3ea28bce48896c8e69c1e79611cc', size=5969245856, details=ModelDetails(parent_model='', format='gguf', family='qwen25vl', families=['qwen25vl'], parameter_size='8.3B', quantization_level='Q4_K_M'))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()['models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5831d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Ollama models:\n",
      "Error listing models: 'name'\n",
      "Checking if llama3.2:1b-text-q2_K is available...\n",
      "‚ùå Error checking model llama3.2:1b-text-q2_K: 'name'\n",
      "\n",
      "‚ö†Ô∏è  llama3.2:1b-text-q2_K model not available. Please pull it first.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "# First, let's check if we have the model available\n",
    "def list_available_models():\n",
    "    try:\n",
    "        models = ollama.list()\n",
    "        print(\"Available Ollama models:\")\n",
    "        for model in models['models']:\n",
    "            print(f\"- {model['name']} (Size: {model['size'] / 1024**3:.1f} GB)\")\n",
    "        return models\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "        return None\n",
    "\n",
    "# Check if the specific model is available\n",
    "def check_model_available(model_name=\"llama3.2:1b-text-q2_K\"):\n",
    "    try:\n",
    "        print(f\"Checking if {model_name} is available...\")\n",
    "        models = ollama.list()\n",
    "        model_names = [model['name'] for model in models['models']]\n",
    "        \n",
    "        if model_name in model_names:\n",
    "            print(f\"‚úÖ Model {model_name} is available and ready to use\")\n",
    "            # Get model details\n",
    "            for model in models['models']:\n",
    "                if model['name'] == model_name:\n",
    "                    print(f\"   - Parameter size: {model['details']['parameter_size']}\")\n",
    "                    print(f\"   - Quantization: {model['details']['quantization_level']}\")\n",
    "                    print(f\"   - Size: {model['size'] / 1024**3:.1f} GB\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Model {model_name} not found\")\n",
    "            print(\"Available models:\")\n",
    "            for name in model_names:\n",
    "                print(f\"   - {name}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking model {model_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# List available models and check our target model\n",
    "models_info = list_available_models()\n",
    "model_available = check_model_available(\"llama3.2:1b-text-q2_K\")\n",
    "\n",
    "if model_available:\n",
    "    print(f\"\\nüéâ Ready to use llama3.2:1b-text-q2_K model!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  llama3.2:1b-text-q2_K model not available. Please pull it first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fcc611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chatbot initialized with model: llama3.2:1b-text-q2_K\n",
      "Ready to chat! Use chatbot.chat('your message') to start chatting.\n"
     ]
    }
   ],
   "source": [
    "class LlamaChatbot:\n",
    "    def __init__(self, model_name=\"llama3.2:1b-text-q2_K\"):\n",
    "        \"\"\"Initialize the chatbot with the specified Llama model\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.conversation_history = []\n",
    "        self.system_prompt = \"\"\"You are a helpful, friendly AI assistant. You provide accurate and helpful responses while being conversational and engaging. Keep your responses concise but informative.\"\"\"\n",
    "        \n",
    "    def add_system_prompt(self, prompt):\n",
    "        \"\"\"Add or update the system prompt\"\"\"\n",
    "        self.system_prompt = prompt\n",
    "        \n",
    "    def chat(self, user_message, max_tokens=150, temperature=0.7, stream=False):\n",
    "        \"\"\"Send a message to the chatbot and get a response\"\"\"\n",
    "        try:\n",
    "            # Add user message to history\n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "            \n",
    "            # Prepare messages for the model (include system prompt and conversation history)\n",
    "            messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "            messages.extend(self.conversation_history)\n",
    "            \n",
    "            # Get response from Ollama\n",
    "            response = ollama.chat(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                options={\n",
    "                    \"temperature\": temperature,\n",
    "                    \"num_predict\": max_tokens\n",
    "                },\n",
    "                stream=stream\n",
    "            )\n",
    "            \n",
    "            if stream:\n",
    "                # Handle streaming response\n",
    "                full_response = \"\"\n",
    "                for chunk in response:\n",
    "                    if 'message' in chunk and 'content' in chunk['message']:\n",
    "                        content = chunk['message']['content']\n",
    "                        print(content, end='', flush=True)\n",
    "                        full_response += content\n",
    "                print()  # New line after streaming\n",
    "                bot_message = full_response\n",
    "            else:\n",
    "                # Handle regular response\n",
    "                bot_message = response['message']['content']\n",
    "            \n",
    "            # Add bot response to history\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "            \n",
    "            return bot_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error generating response: {e}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"Conversation history cleared.\")\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"Get the conversation history\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def save_conversation(self, filename):\n",
    "        \"\"\"Save conversation to a JSON file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump({\n",
    "                    \"model\": self.model_name,\n",
    "                    \"system_prompt\": self.system_prompt,\n",
    "                    \"conversation\": self.conversation_history\n",
    "                }, f, indent=2)\n",
    "            print(f\"Conversation saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving conversation: {e}\")\n",
    "    \n",
    "    def load_conversation(self, filename):\n",
    "        \"\"\"Load conversation from a JSON file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                self.conversation_history = data.get(\"conversation\", [])\n",
    "                self.system_prompt = data.get(\"system_prompt\", self.system_prompt)\n",
    "            print(f\"Conversation loaded from {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading conversation: {e}\")\n",
    "\n",
    "# Initialize the chatbot\n",
    "chatbot = LlamaChatbot()\n",
    "print(f\"‚úÖ Chatbot initialized with model: {chatbot.model_name}\")\n",
    "print(\"Ready to chat! Use chatbot.chat('your message') to start chatting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44cb3423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Testing the Llama 3.2:1b Chatbot\n",
      "==================================================\n",
      "Error generating response: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "User: Hello! What's your name and what can you do?\n",
      "Bot: Error generating response: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "\n",
      "Error generating response: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "User: Can you help me write a simple Python function to calculate the factorial of a number?\n",
      "Bot: Error generating response: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "\n",
      "Error generating response: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "User: What's the weather like today?\n",
      "Bot: Error generating response: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "\n",
      "Conversation History:\n",
      "Total messages: 3\n"
     ]
    }
   ],
   "source": [
    "# Test the chatbot with a simple conversation\n",
    "print(\"ü§ñ Testing the Llama 3.2:1b Chatbot\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test basic conversation\n",
    "response1 = chatbot.chat(\"Hello! What's your name and what can you do?\")\n",
    "print(f\"User: Hello! What's your name and what can you do?\")\n",
    "print(f\"Bot: {response1}\")\n",
    "print()\n",
    "\n",
    "# Test follow-up question\n",
    "response2 = chatbot.chat(\"Can you help me write a simple Python function to calculate the factorial of a number?\")\n",
    "print(f\"User: Can you help me write a simple Python function to calculate the factorial of a number?\")\n",
    "print(f\"Bot: {response2}\")\n",
    "print()\n",
    "\n",
    "# Test another topic\n",
    "response3 = chatbot.chat(\"What's the weather like today?\")\n",
    "print(f\"User: What's the weather like today?\")\n",
    "print(f\"Bot: {response3}\")\n",
    "print()\n",
    "\n",
    "print(\"Conversation History:\")\n",
    "print(f\"Total messages: {len(chatbot.get_history())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff2f2cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Chatbot is ready!\n",
      "\n",
      "Available functions:\n",
      "- chatbot.chat('your message') - Single message\n",
      "- interactive_chat() - Start interactive session\n",
      "- create_specialized_chatbot('specialty') - Create specialized bot\n",
      "- chatbot.clear_history() - Clear conversation history\n",
      "- chatbot.save_conversation('filename.json') - Save conversation\n",
      "‚úÖ Created programmer chatbot\n"
     ]
    }
   ],
   "source": [
    "# Interactive chatting function\n",
    "def interactive_chat():\n",
    "    \"\"\"Start an interactive chat session\"\"\"\n",
    "    print(\"ü§ñ Interactive Chat Mode\")\n",
    "    print(\"Type 'quit' to exit, 'clear' to clear history, 'save' to save conversation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nYou: \").strip()\n",
    "            \n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"Goodbye! üëã\")\n",
    "                break\n",
    "            elif user_input.lower() == 'clear':\n",
    "                chatbot.clear_history()\n",
    "                continue\n",
    "            elif user_input.lower() == 'save':\n",
    "                filename = input(\"Enter filename (e.g., 'chat_log.json'): \")\n",
    "                chatbot.save_conversation(filename)\n",
    "                continue\n",
    "            elif user_input.lower() == 'history':\n",
    "                print(f\"\\nConversation has {len(chatbot.get_history())} messages\")\n",
    "                continue\n",
    "            elif user_input == '':\n",
    "                continue\n",
    "                \n",
    "            # Get bot response\n",
    "            response = chatbot.chat(user_input, stream=True)\n",
    "            print(f\"\\nBot: \", end=\"\")\n",
    "            # Response is printed via streaming in the chat method\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nChat interrupted. Goodbye! üëã\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "\n",
    "# Example of customizing the system prompt\n",
    "def create_specialized_chatbot(specialty=\"general\"):\n",
    "    \"\"\"Create a chatbot with specialized knowledge\"\"\"\n",
    "    specialized_prompts = {\n",
    "        \"programmer\": \"You are an expert programmer and software engineer. You help with coding problems, debugging, and best practices across multiple programming languages.\",\n",
    "        \"teacher\": \"You are a patient and knowledgeable teacher. You explain concepts clearly, provide examples, and adapt your explanations to the student's level.\",\n",
    "        \"creative\": \"You are a creative writing assistant. You help with storytelling, poetry, creative ideas, and imaginative content.\",\n",
    "        \"analyst\": \"You are a data analyst and researcher. You help analyze information, provide insights, and explain complex topics in a clear, structured way.\"\n",
    "    }\n",
    "    \n",
    "    if specialty in specialized_prompts:\n",
    "        specialized_bot = LlamaChatbot()\n",
    "        specialized_bot.add_system_prompt(specialized_prompts[specialty])\n",
    "        print(f\"‚úÖ Created {specialty} chatbot\")\n",
    "        return specialized_bot\n",
    "    else:\n",
    "        print(\"Available specialties: programmer, teacher, creative, analyst\")\n",
    "        return None\n",
    "\n",
    "print(\"üöÄ Chatbot is ready!\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"- chatbot.chat('your message') - Single message\")\n",
    "print(\"- interactive_chat() - Start interactive session\")\n",
    "print(\"- create_specialized_chatbot('specialty') - Create specialized bot\")\n",
    "print(\"- chatbot.clear_history() - Clear conversation history\")\n",
    "print(\"- chatbot.save_conversation('filename.json') - Save conversation\")\n",
    "\n",
    "# Example of creating a specialized chatbot\n",
    "programmer_bot = create_specialized_chatbot(\"programmer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d3335",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d0dfed5",
   "metadata": {},
   "source": [
    "#### Simple RAG System with Ollama Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebbcf5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7cc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf5e0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SimpleRAG initialized with LLM: llama3.2:1b-text-q2_K and Embeddings: nomic-embed-text\n",
      "\n",
      "üöÄ Simple RAG system is ready!\n",
      "Use rag.add_documents([list_of_docs]) to add knowledge\n",
      "Use rag.query('your question') to ask questions\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self, llm_model=\"llama3.2:1b-text-q2_K\", embedding_model=\"nomic-embed-text\"):\n",
    "        self.llm_model = llm_model\n",
    "        self.embedding_model = embedding_model\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "        print(f\"‚úÖ SimpleRAG initialized with LLM: {llm_model} and Embeddings: {embedding_model}\")\n",
    "    \n",
    "    def add_documents(self, docs):\n",
    "        \"\"\"Add documents to the knowledge base\"\"\"\n",
    "        print(f\"Adding {len(docs)} documents to knowledge base...\")\n",
    "        \n",
    "        for i, doc in enumerate(docs):\n",
    "            # Store document\n",
    "            self.documents.append(doc)\n",
    "            \n",
    "            # Generate embedding\n",
    "            try:\n",
    "                response = ollama.embeddings(\n",
    "                    model=self.embedding_model,\n",
    "                    prompt=doc\n",
    "                )\n",
    "                embedding = response['embedding']\n",
    "                self.embeddings.append(embedding)\n",
    "                print(f\"‚úÖ Document {i+1}/{len(docs)} processed\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing document {i+1}: {e}\")\n",
    "                self.documents.pop()  # Remove document if embedding failed\n",
    "        \n",
    "        print(f\"üìö Knowledge base now contains {len(self.documents)} documents\")\n",
    "    \n",
    "    def search_documents(self, query, top_k=3):\n",
    "        \"\"\"Search for most relevant documents\"\"\"\n",
    "        if not self.documents:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Get query embedding\n",
    "            query_response = ollama.embeddings(\n",
    "                model=self.embedding_model,\n",
    "                prompt=query\n",
    "            )\n",
    "            query_embedding = query_response['embedding']\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = []\n",
    "            for doc_embedding in self.embeddings:\n",
    "                similarity = cosine_similarity(\n",
    "                    [query_embedding], \n",
    "                    [doc_embedding]\n",
    "                )[0][0]\n",
    "                similarities.append(similarity)\n",
    "            \n",
    "            # Get top-k most similar documents\n",
    "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "            \n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                results.append({\n",
    "                    'document': self.documents[idx],\n",
    "                    'similarity': similarities[idx],\n",
    "                    'index': idx\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching documents: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def generate_answer(self, query, context_docs):\n",
    "        \"\"\"Generate answer using LLM with context\"\"\"\n",
    "        if not context_docs:\n",
    "            context = \"No relevant information found.\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['document'] for doc in context_docs])\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following context, answer the question. If the context doesn't contain enough information to answer the question, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=self.llm_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0.1, \"num_predict\": 200}\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error generating answer: {e}\"\n",
    "    \n",
    "    def query(self, question, top_k=3, show_sources=True):\n",
    "        \"\"\"Main RAG query function\"\"\"\n",
    "        print(f\"üîç Searching for: {question}\")\n",
    "        \n",
    "        # Search for relevant documents\n",
    "        relevant_docs = self.search_documents(question, top_k)\n",
    "        \n",
    "        if show_sources and relevant_docs:\n",
    "            print(f\"\\nüìñ Found {len(relevant_docs)} relevant documents:\")\n",
    "            for i, doc in enumerate(relevant_docs):\n",
    "                print(f\"{i+1}. (Similarity: {doc['similarity']:.3f}) {doc['document'][:100]}...\")\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.generate_answer(question, relevant_docs)\n",
    "        \n",
    "        print(f\"\\nü§ñ Answer: {answer}\")\n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'sources': relevant_docs\n",
    "        }\n",
    "\n",
    "# Initialize RAG system\n",
    "rag = SimpleRAG()\n",
    "print(\"\\nüöÄ Simple RAG system is ready!\")\n",
    "print(\"Use rag.add_documents([list_of_docs]) to add knowledge\")\n",
    "print(\"Use rag.query('your question') to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67407adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for: What is the Capital city of India \n",
      "\n",
      "ü§ñ Answer: Error generating answer: llama runner process has terminated: signal: broken pipe (status code: 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'Error generating answer: llama runner process has terminated: signal: broken pipe (status code: 500)',\n",
       " 'sources': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.query('What is the Capital city of India ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc1068a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Adding sample documents to the knowledge base...\n",
      "Adding 6 documents to knowledge base...\n",
      "‚ùå Error processing document 1: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "‚ùå Error processing document 2: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "‚ùå Error processing document 3: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "‚ùå Error processing document 4: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "‚ùå Error processing document 5: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "‚ùå Error processing document 6: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "üìö Knowledge base now contains 0 documents\n"
     ]
    }
   ],
   "source": [
    "# Sample documents for testing\n",
    "sample_docs = [\n",
    "    \"Python is a high-level programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991.\",\n",
    "    \n",
    "    \"Machine Learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed.\",\n",
    "    \n",
    "    \"The Ollama platform allows you to run large language models locally on your machine. It supports various models like Llama, Mistral, and others.\",\n",
    "    \n",
    "    \"RAG (Retrieval-Augmented Generation) combines information retrieval with text generation to provide more accurate and contextual responses.\",\n",
    "    \n",
    "    \"Cotton is a natural fiber that grows around the seeds of cotton plants. It's one of the most important agricultural crops worldwide and is used primarily in textile production.\",\n",
    "    \n",
    "    \"Embeddings are dense vector representations of text that capture semantic meaning. They allow computers to understand the similarity between different pieces of text.\"\n",
    "]\n",
    "\n",
    "# Add sample documents to RAG\n",
    "print(\"üìö Adding sample documents to the knowledge base...\")\n",
    "rag.add_documents(sample_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47108642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing the RAG system with sample queries...\n",
      "============================================================\n",
      "üîç Searching for: What is Python programming language?\n",
      "\n",
      "ü§ñ Answer: Error generating answer: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "\n",
      "============================================================\n",
      "üîç Searching for: How does machine learning work?\n",
      "\n",
      "ü§ñ Answer: Error generating answer: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "\n",
      "============================================================\n",
      "üîç Searching for: Tell me about cotton and its uses\n",
      "\n",
      "ü§ñ Answer: Error generating answer: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "\n",
      "============================================================\n",
      "üîç Searching for: What is the weather like today?\n",
      "\n",
      "ü§ñ Answer: Error generating answer: llama runner process has terminated: signal: broken pipe (status code: 500)\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system\n",
    "print(\"üß™ Testing the RAG system with sample queries...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test query 1\n",
    "result1 = rag.query(\"What is Python programming language?\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Test query 2\n",
    "result2 = rag.query(\"How does machine learning work?\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Test query 3\n",
    "result3 = rag.query(\"Tell me about cotton and its uses\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Test query 4 - something not in knowledge base\n",
    "result4 = rag.query(\"What is the weather like today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f887212",
   "metadata": {},
   "source": [
    "#### Testing with Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54716da0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62b23e66",
   "metadata": {},
   "source": [
    "#### Simple Q&A Chatbot with Llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c36e0e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Chatbot initialized with model: llama3.2:1b-text-q2_K\n",
      "‚ùå Error: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "Make sure Ollama is running and the model is pulled.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "class SimpleQAChatbot:\n",
    "    def __init__(self, model_name=\"llama3.2:1b-text-q2_K\"):\n",
    "        \"\"\"Initialize a simple Q&A chatbot\"\"\"\n",
    "        self.model_name = model_name\n",
    "        print(f\"ü§ñ Chatbot initialized with model: {model_name}\")\n",
    "        \n",
    "        # Test if model is available\n",
    "        try:\n",
    "            test_response = ollama.chat(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                options={\"num_predict\": 5}\n",
    "            )\n",
    "            print(\"‚úÖ Model is ready and responsive!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            print(\"Make sure Ollama is running and the model is pulled.\")\n",
    "    \n",
    "    def ask(self, question, max_tokens=150):\n",
    "        \"\"\"Ask a question and get an answer\"\"\"\n",
    "        try:\n",
    "            print(f\"\\n‚ùì Question: {question}\")\n",
    "            print(\"ü§ñ Thinking...\")\n",
    "            \n",
    "            response = ollama.chat(\n",
    "                model=self.model_name,\n",
    "                messages=[{\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are a helpful assistant. Provide clear, concise, and accurate answers.\"\n",
    "                }, {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": question\n",
    "                }],\n",
    "                options={\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"num_predict\": max_tokens\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            answer = response['message']['content']\n",
    "            print(f\"üí¨ Answer: {answer}\")\n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error: {e}\"\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            return error_msg\n",
    "    \n",
    "    def chat_loop(self):\n",
    "        \"\"\"Start an interactive chat session\"\"\"\n",
    "        print(\"\\nüéØ Interactive Q&A Mode\")\n",
    "        print(\"Type 'quit' or 'exit' to stop chatting\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                question = input(\"\\nüôã Your question: \").strip()\n",
    "                \n",
    "                if question.lower() in ['quit', 'exit', 'bye']:\n",
    "                    print(\"üëã Goodbye! Thanks for chatting!\")\n",
    "                    break\n",
    "                    \n",
    "                if not question:\n",
    "                    print(\"Please ask a question or type 'quit' to exit.\")\n",
    "                    continue\n",
    "                \n",
    "                self.ask(question)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nüëã Chat interrupted. Goodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Unexpected error: {e}\")\n",
    "\n",
    "# Initialize the simple chatbot\n",
    "qa_bot = SimpleQAChatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3f913d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing the Q&A Chatbot\n",
      "==================================================\n",
      "\n",
      "--- Test 1 ---\n",
      "\n",
      "‚ùì Question: What is artificial intelligence?\n",
      "ü§ñ Thinking...\n",
      "‚ùå Error: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "------------------------------\n",
      "\n",
      "--- Test 2 ---\n",
      "\n",
      "‚ùì Question: How does machine learning work?\n",
      "ü§ñ Thinking...\n",
      "‚ùå Error: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "------------------------------\n",
      "\n",
      "--- Test 3 ---\n",
      "\n",
      "‚ùì Question: What are the benefits of Python programming?\n",
      "ü§ñ Thinking...\n",
      "‚ùå Error: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "------------------------------\n",
      "\n",
      "--- Test 4 ---\n",
      "\n",
      "‚ùì Question: Explain what is cotton and its uses\n",
      "ü§ñ Thinking...\n",
      "‚ùå Error: llama runner process has terminated: signal: broken pipe (status code: 500)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the chatbot with some sample questions\n",
    "print(\"üß™ Testing the Q&A Chatbot\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does machine learning work?\", \n",
    "    \"What are the benefits of Python programming?\",\n",
    "    \"Explain what is cotton and its uses\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    qa_bot.ask(question)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "951c5da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Ready to use!\n",
      "Use qa_bot.ask('your question') for single questions\n",
      "Use qa_bot.chat_loop() for interactive chat mode\n"
     ]
    }
   ],
   "source": [
    "# Interactive mode - uncomment the line below to start chatting\n",
    "# qa_bot.chat_loop()\n",
    "\n",
    "# Or use single questions like this:\n",
    "# qa_bot.ask(\"Your question here\")\n",
    "\n",
    "print(\"\\nüéØ Ready to use!\")\n",
    "print(\"Use qa_bot.ask('your question') for single questions\")  \n",
    "print(\"Use qa_bot.chat_loop() for interactive chat mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bfd1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc828115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cotton_bz_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
